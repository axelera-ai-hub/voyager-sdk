# Copyright Axelera AI, 2024
# Evaluator for metric measurement from data generated by pipeline

import dataclasses
import datetime
import json
import logging
import sys
from typing import Any, Dict, List, Optional, Type, TypeVar

from axelera import types
from axelera.app import meta

from . import utils

LOG = logging.getLogger(__name__)
T = TypeVar("T")


@dataclasses.dataclass
class MetricSummary:
    """Contains the information about a metric that can be written in a report"""

    generator_name: str
    metric_name: str
    aggregator_name: str
    value: float
    schema_url: Optional[str]

    def _to_json(self) -> str:
        return json.dumps(dataclasses.asdict(self), indent=4)

    @classmethod
    def _from_dict(cls: Type[T], metrics_dict: Dict[Any, Any]) -> T:
        field_names = {f.name for f in dataclasses.fields(cls)}
        return cls(
            **{k: v for k, v in metrics_dict.items() if k in field_names}
        )  # type: ignore [call-arg]

    def dump(self, filename: str) -> None:
        """Writes the summary to a JSON file

        Arguments:
            filename: path to the JSON file to write to
        """
        with open(filename, "wb") as f:
            f.write(self._to_json().encode("utf-8"))

    @classmethod
    def load(cls: Any, filename: str) -> Any:
        """Loads the summary from a JSON filename

        Arguments:
            filename: path to the JSON file to load from
        """
        with open(filename, "rb") as f:
            metric_dict = json.load(f)
            return cls._from_dict(metric_dict)


@dataclasses.dataclass
class EvaluatorSummary:
    """Contains the information about an evaluation run"""

    model_name: str
    dataset_name: str
    date: str  # needs to be in ISO 8601 format
    inference_time_ms: float  # in milliseconds
    compute_time_ms: float  # in milliseconds
    metric_summaries: List[MetricSummary]
    schema_url: Optional[str] = None

    def dump(self, filename: str) -> None:
        """Writes the summary to a JSON file

        Arguments:
            filename: path to the JSON file to write to
        """
        with open(filename, "wb") as f:
            f.write(self._to_json().encode("utf-8"))

    @classmethod
    def load(cls: Any, filename: str) -> Any:
        """Loads the summary from a JSON filename

        Arguments:
            filename: path to the JSON file to load from
        """

        with open(filename, "rb") as f:
            metric_dict = json.load(f)
            return cls._from_dict(metric_dict)

    def _to_json(self) -> str:
        return json.dumps(dataclasses.asdict(self), indent=4)

    @classmethod
    def _from_dict(cls: Type[T], metrics_dict: Dict[Any, Any]) -> T:
        field_names = {f.name for f in dataclasses.fields(cls)}
        filtered_dict = {k: v for k, v in metrics_dict.items() if k in field_names}
        # convert the nested metric summary dicts to objects
        metric_dicts = filtered_dict["metric_summaries"]
        metric_summaries = [MetricSummary._from_dict(metric_dict) for metric_dict in metric_dicts]
        # replace the metric summary dicts with the metric summary objects
        filtered_dict["metric_summaries"] = metric_summaries

        return cls(**filtered_dict)  # type: ignore [call-arg]


@dataclasses.dataclass
class AxEvaluator:
    model_name: str
    dataset_name: str
    task_category: dataclasses.InitVar[types.TaskCategory]
    task_name: str
    dataset_config: dataclasses.InitVar[Dict[str, Any]]
    dataset: dataclasses.InitVar[Any]
    # master_task is assigned only if measuring from a cascade pipeline
    master_task: Optional[str] = None

    processing_time: List[float] = dataclasses.field(
        default_factory=list
    )  # processing time for each sample
    evaluator: Any = dataclasses.field(repr=False, default=None)

    def __post_init__(
        self,
        task_category: types.TaskCategory,
        dataset_config: Dict[str, Any],
        dataset: Any = None,
    ):
        if self.evaluator:
            if not isinstance(self.evaluator, types.Evaluator):
                raise ValueError(
                    f"Customer evaluator must be an instance of EvaluatorInterface: {self.evaluator}"
                )
            LOG.debug(f"Using customer evaluator: {self.evaluator}")
        else:
            # TODO: support default evaluator based on task category
            raise NotImplementedError("Default evaluator not implemented yet")
        if self.master_task:
            self._num_no_master_detections = 0

    def evaluate_metrics(self, inference_time_sec):
        """Evaluates the desired metrics and produce a report

        Returns:
            a summary of the inference evaluation results
        """
        with utils.catchtime() as eval_elapsed_sec:
            self.eval_result = self.evaluator.collect_metrics()
        if not isinstance(self.eval_result, types.EvalResult):
            raise TypeError(
                f"collect_metrics should return an instance of EvalResult, got {type(self.eval_result)}"
            )
        eval_elapsed_sec = eval_elapsed_sec.time + sum(self.processing_time)
        metric_summaries = [
            MetricSummary(
                generator_name="",
                metric_name=metric_name,
                aggregator_name=aggregator_name,
                value=metric_value,
                schema_url="",
            )
            for metric_name, aggregator_name, metric_value in self.eval_result
        ]

        eval_summary = EvaluatorSummary(
            model_name=self.model_name,
            dataset_name=self.dataset_name,
            date=datetime.datetime.now().isoformat(sep=' '),
            inference_time_ms=round(inference_time_sec * 1e3, 3),
            compute_time_ms=round(eval_elapsed_sec * 1e3, 3),
            metric_summaries=metric_summaries,
            schema_url="",
        )
        self.summary = eval_summary
        return eval_summary

    def name_matches(self, m, metric_name):
        names = [n.NAME for n in self.eval_result[metric_name]]
        return (
            (m.metric_name == names[0] and m.aggregator_name == names[1])
            or m.metric_name.lower() == metric_name
            or m.metric_name.lower().replace("_", "") == metric_name.replace("_", "")
        )

    def write_metrics(self, stream=sys.stdout):
        if not hasattr(self, 'summary'):
            raise ValueError("No summary available. Please run evaluate_metrics() first.")
        metric_names = [metric.upper() for metric, _, _ in self.eval_result]
        max_metric_name_length = max(len(name) for name in metric_names)
        space = max_metric_name_length + 2
        model_padding = space - len("Model:")
        dataset_padding = space - len("Dataset:")
        date_padding = space - len("Date:")
        inference_padding = space - len("Inference Time:")
        compute_padding = space - len("Evaluation Time:")
        stream.write(f"Model:{' ' * model_padding} {self.summary.model_name}\n")
        stream.write(f"Dataset:{' ' * dataset_padding} {self.summary.dataset_name}\n")
        stream.write(f"Date:{' ' * date_padding} {self.summary.date}\n")
        stream.write(
            f"Inference Time:{' ' * inference_padding} {self.summary.inference_time_ms:.2f}ms\n"
        )
        stream.write(
            f"Evaluation Time:{' ' * compute_padding} {self.summary.compute_time_ms:.2f}ms\n"
        )

        if self.master_task:
            stream.write(
                f"Number of no {self.master_task} detection:{' ' * 3} {self._num_no_master_detections}\n"
            )

        if self.eval_result.custom_print_function:
            self.eval_result.print_results(stream)
        else:
            stream.write("Evaluation Metrics:\n")
            max_metric_length = max(
                len(f"{metric}_{agg_name}" if agg_name else metric)
                for metric, agg_name, _ in self.eval_result
            )
            max_value_length = max(
                len(f"{value:.2%}" if isinstance(value, float) else str(value))
                for _, _, value in self.eval_result
            )

            # Calculate the total width including the border characters
            total_width = max_metric_length + max_value_length + 7  # 7 for "| ", " | ", and " |"

            stream.write("=" * total_width + "\n")
            for metric, agg_name, value in self.eval_result:
                formatted_metric = f"{metric}_{agg_name}" if agg_name else metric
                metric_padding = ' ' * (max_metric_length - len(formatted_metric))
                formatted_value = f"{value:.2%}" if isinstance(value, float) else str(value)
                value_padding = ' ' * (max_value_length - len(formatted_value))
                stream.write(
                    f"| {formatted_metric}{metric_padding} | {formatted_value}{value_padding} |\n"
                )

            stream.write("=" * total_width + "\n")

            if key_metric := self.eval_result.key_metric:
                value = (
                    f"{key_metric: .4f}"
                    if isinstance(self.eval_result.key_metric, float)
                    else key_metric
                )
                stream.write(
                    f"\033[1mKey Metric ({self.eval_result._key_metric}_{self.eval_result._key_aggregator}): {value}\033[0m\n"
                )
            else:
                LOG.warning(f"No key metric registered in {self.eval_result}")

    def append_new_sample(self, ax_meta: meta.AxMeta):
        if self.master_task:
            try:
                model_metas = ax_meta.aggregate_leaf_metas(self.master_task, self.task_name)
            except meta.NoMasterDetectionsError:
                self._num_no_master_detections += 1
                return
        else:
            model_metas = [ax_meta[self.task_name]]

        try:
            with utils.catchtime() as eval_elapsed_sec:
                for model_meta in model_metas:
                    self.evaluator.process_meta(model_meta)
            self.processing_time.append(eval_elapsed_sec.time)
        except Exception as e:
            LOG.error(f"Error in process_meta: {str(e)}")
            raise
